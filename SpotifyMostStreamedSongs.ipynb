{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4507e463",
   "metadata": {},
   "source": [
    "<h2> Most Streamed Spotify Songs Analysis </h2>\n",
    "<p> In this notebook I run through some exploratory work with this dataset and then I begin to apply some more in depth analysis to produce a model that predicts the amount of streams a song recieves. As you look through the code I want you to keep in mind that this model is still being worked on in terms of performance. I suppose you can call it somewhat of a start. I enjoy applying my skills and this is, in my opinion, the best way for me to do it. \n",
    "    \n",
    "<p> I tried my best to comment and explain my train of thought, but I apologize if I lose you along the way. My brain works in an interesting way at times. Cheers! </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0a8c3",
   "metadata": {},
   "source": [
    "<h3> Initial Packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc94906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to load the dataset using 'ISO-8859-1' encoding\n",
    "spotify_raw_data = pd.read_csv(r\"C:\\Users\\Steph\\Desktop\\Python\\spotify-2023.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "spotify_raw_data.head()\n",
    "\n",
    "# Assigning data to dataframe\n",
    "df = pd.DataFrame(spotify_raw_data)\n",
    "print(df.shape) # (953 , 24)\n",
    "\n",
    "# Check for nulls\n",
    "print(\"\\nMissing Values:\")\n",
    "print(spotify_raw_data.isnull().sum()) # [in_shazam_charts],[key] contain null values\n",
    "\n",
    "# Summary statistics \n",
    "print(\"\\nSummary Statistics:\")\n",
    "data_sumstats = spotify_raw_data.describe(include='all')\n",
    "data_sumstats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1be98",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>\n",
    "<p> In these next few steps I aim to remove the nulls as well as any potential duplicates or data type errors. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at missing values again \n",
    "df.isnull().sum()\n",
    "\n",
    "# Decide whether to drop data or input missing data\n",
    "# In_shazam_charts empty values can be taken as 0\n",
    "# Key is more difficult and best option is to 1) Find the data 2) Remove rows and the consequences that come with less data\n",
    "  # Moving forward with removing rows for now\n",
    "    \n",
    "# in_shazam_charts\n",
    "df['in_shazam_charts'] = df['in_shazam_charts'].fillna(0)\n",
    "\n",
    "# using dropna from pandas to remove nulls\n",
    "df = df.dropna()\n",
    "df.isnull().sum() \n",
    "\n",
    "# Expecting a 95 row decrease - validating\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.drop_duplicates() \n",
    "\n",
    "# Checking if rows were dropped\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3138ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at missing values again to validate above\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22028f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at datatypes to see what needs to be changed\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e674de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below are expected to be numerical\n",
    "\n",
    "df['streams'] = pd.to_numeric(df['streams'], errors = 'coerce')\n",
    "df = df.dropna() # Do not want to input 0 for streams. Will be dropping.\n",
    "\n",
    "# numbers are objects due to commas\n",
    "df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '')\n",
    "df = df.dropna() # Do not want to input 0 for streams. Will be dropping.\n",
    "df['in_shazam_charts'] = df['in_shazam_charts'].astype(int)\n",
    "\n",
    "# numbers are objects due to commas\n",
    "df['in_deezer_playlists'] = df['in_deezer_playlists'].str.replace(',', '')\n",
    "df = df.dropna() # Do not want to input 0 for streams. Will be dropping.\n",
    "df['in_deezer_playlists'] = df['in_deezer_playlists'].astype(int)\n",
    "\n",
    "# Looking at datatypes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at null values\n",
    "df.isnull().sum()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ac588",
   "metadata": {},
   "source": [
    "<h2> Categorical Variables </h2>\n",
    "<p> I like to add indicator variables for my categories. I may not use them all, but they are good to add in the dataset just in case </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258456d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Features\n",
    " # I like to use get dummies because of the naming convention\n",
    "\n",
    "# Categorizing [mode]\n",
    "df = pd.get_dummies(df, columns=[\"mode\"], prefix = \"mode\")\n",
    "\n",
    "# Categorizing [key]\n",
    "df = pd.get_dummies(df, columns=[\"key\"], prefix = \"key\")\n",
    "\n",
    "# Converting from bool to int\n",
    "for col in df.columns[df.columns.get_loc('mode_Minor'):]:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8ee6b",
   "metadata": {},
   "source": [
    "<h2>Preliminary Data Exploration</h2>\n",
    "<p> Here I will be running some simple graphs to give me some visuals regarding the data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94221b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics : Distributions\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Setting features of interest to a single variable\n",
    "features_set = ['bpm','danceability_%', 'valence_%', 'energy_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%']\n",
    "\n",
    "# Defining axes\n",
    "fig, axes = plt.subplots(nrows=len(features_set), figsize = (15,15))\n",
    "\n",
    "# Plot the ditribution\n",
    "for i, feature in enumerate(features_set):\n",
    "    sns.histplot(df[feature], ax=axes[i], bins=30, kde=True)\n",
    "    axes[i].set_title(f'Distribution of {feature}', fontsize=11)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "# set the spacing between subplots\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the top 10 songs based on total streams\n",
    "streaming_df = df.sort_values(by='streams', ascending = False).head(10) # limiting to the top most 10 results\n",
    "\n",
    "# Adding artist name to song name\n",
    "streaming_df['track_artist'] = streaming_df['track_name'] + \"-\" + streaming_df['artist(s)_name']\n",
    "\n",
    "# visualizing above data \n",
    "print(\"Streams by Song-Artist:\")\n",
    "# print(streaming_df['track_artist'])\n",
    "\n",
    "# Plotting the streams with aritst and song information\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "     x = streaming_df['streams']\n",
    "    ,y = streaming_df['track_artist']\n",
    "    ,palette = \"flare\"\n",
    "    ,orient = 'h'\n",
    "     )\n",
    "plt.title('Top 10 Streams by Song-Artist')\n",
    "plt.xlabel('Streams in Billions')\n",
    "plt.ylabel('Track - Artist')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting total streams by artist\n",
    "streamsxartist_df = df.groupby('artist(s)_name')['streams'].sum().sort_values(ascending=False).head(10) # limiting to the top most 10 results\n",
    "sns.barplot(\n",
    "     x = streamsxartist_df.values\n",
    "    ,y = streamsxartist_df.index\n",
    "    ,palette = \"flare\"\n",
    "    ,orient = 'h'\n",
    "     )\n",
    "plt.title('Top 10 Artists with the Highest Streams')\n",
    "plt.xlabel('Total Streams in Billions')\n",
    "plt.ylabel('Artist(s) Name')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df314689",
   "metadata": {},
   "source": [
    "<h2>Data Analysis</h2>\n",
    "<p> Some discovery work has been done to get to know the dataset a little more. Time to run some analysis! Let's try and create a model that can predict how many streams a song gets. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6ea2c",
   "metadata": {},
   "source": [
    "<h3>Correlation</h3>\n",
    "<p> Before starting any model, I like to look at what features have a high correlation to my dependent variable. This will allow me to understand the relationship between my features and target variable.\n",
    "For example, I am currently assuming that if a song has high streams then that is synonymous to it being in many Spotify playlists. If it's in Spotify playlists, it is a popular song, hence leading to more streams. Or maybe it's the other way around? Either way, a correlation coefficient will help determine how strong of a relationship these variables have with eachother, if at all. Additionally, here you can find variables that may give you some collinearity issues which would lead to those variables not being in the model.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data to remove character values\n",
    "df_corr = df.drop(['track_name', 'artist(s)_name'], axis = 1)\n",
    "\n",
    "# Running a simple pearson correlation on the data that has the 'streams' variable set to only numeric values\n",
    "df_corr.corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d2a4e",
   "metadata": {},
   "source": [
    "<p>As anticipated, the presence of songs in playlists—whether on platforms like Apple Music, Spotify, or Deezer—shows a substantial correlation, roughly around 70%. An intriguing aspect that the available data doesn't clarify is the origin of these playlists. Specifically, it remains uncertain whether these playlists are curated automatically by the platform itself (e.g., Spotify's algorithm) or by individual users.</p>\n",
    "\n",
    "<p>This distinction holds significance because it raises questions about listener engagement. For instance, when a song achieves chart-topping status or surges in popularity, platforms like Spotify might automatically place it in playlists. Conversely, if a user includes a song in their playlist, it suggests a deliberate choice. This prompts an inquiry: Do songs within user-curated playlists receive more plays compared to those in platform-curated playlists? These are intriguing questions that could offer valuable insights into music listening behavior. However, for the purpose of this analysis and the type of data we are limited to here, we will proceed without addressing them.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e731f",
   "metadata": {},
   "source": [
    "<h3>Scaling the Target Variable</h3> \n",
    "\n",
    "<p>When examining the data, it becomes apparent that the target variable, 'Streams,' is on a significantly larger scale compared to the feature variables. In such scenarios, it is common practice to consider scaling the variables to ensure uniformity in magnitude.</p>\n",
    "\n",
    "<p>In the following steps, a logarithmic transformation will be applied to the target variable, and the feature variables will be normalized. It's important to note that while this transformation may impact the ease of interpreting coefficients, it often leads to enhanced performance of machine learning models.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X : dropping variables that are not of interest at the moment\n",
    "X = df.drop(['track_name', 'artist(s)_name','streams','released_year','released_month','released_day'], axis = 1)\n",
    "X_Numeric = (X.loc[:, :'speechiness_%'])\n",
    "\n",
    "# Define y - using log transformation due to large values\n",
    "df['streams_log'] = np.log(df['streams'])\n",
    "y = df.streams_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0eed6",
   "metadata": {},
   "source": [
    "<p>Next, an analysis of the relationship between the target variables and the feature variables will be conducted. The following code runs a loop to create graphs for each feature in comparison to the target variable, offering a quick visual overview. In addition, a correlation matrix will be generated for reference.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your independent variable and y is your dependent variable\n",
    "\n",
    "dependent_variable = 'streams_log'\n",
    "\n",
    "# Get the list of independent variable column names\n",
    "independent_variables = df.columns\n",
    "\n",
    "# Setting up subplots\n",
    "\n",
    "# Calculate the number of independent variables and rows for subplots\n",
    "num_features = len(independent_variables)\n",
    "\n",
    "# adds the whole number of rows (calculated in step 1) to either 0 or 1, \n",
    "# depending on whether there is an odd number of features left. \n",
    "# If there's an odd number of features, it adds an extra row to accommodate \n",
    "# the last feature in its own row. If there's an even number of features, it doesn't add an extra row.\n",
    "num_rows = num_features // 2 + (num_features % 2)\n",
    "\n",
    "# Create a figure with subplots for multiple scatterplots\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(20, 3*num_rows))\n",
    "\n",
    "# Iterate through independent variables and create scatterplots\n",
    "for i, feature in enumerate(independent_variables):\n",
    "    # Calculate the row and column for the current subplot\n",
    "    row, col = divmod(i, 2)\n",
    "    ax = axes[row, col]  # Get the current subplot\n",
    "    \n",
    "     # Create a scatterplot of the current independent variable against the dependent variable\n",
    "    ax.scatter(df[feature], df[dependent_variable])\n",
    "    \n",
    "     # Set labels and title for the subplot\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel(dependent_variable)\n",
    "    ax.set_title(f'Scatterplot of {feature} vs. {dependent_variable}')\n",
    "    \n",
    "# Remove any empty subplots if the number of features is odd\n",
    "if num_features % 2 == 1:\n",
    "    fig.delaxes(axes[num_rows-1, 1])\n",
    "\n",
    "# Show the scatterplots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591415b",
   "metadata": {},
   "source": [
    "<h3> Normalizing Features </h3>\n",
    "<p> My next step is to also standardize my feautures. This will allow alorithms to run faster. Below we will implement Z-score standardization as well as run some visualizations on the effect of the stardization process. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f11392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing our features using z score\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3619903",
   "metadata": {},
   "source": [
    "<p> The below code allows you to see the way the data changes before, during, and after normalization. Use the feature_indices to change the options. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e76034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the feature indices here\n",
    "feature_indices = [1, 5]  # Change these to the desired feature indices\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mu = np.mean(X_Numeric, axis=0)\n",
    "sigma = np.std(X_Numeric, axis=0)\n",
    "\n",
    "# Calculate X_mean and X_norm\n",
    "X_mean = (X_Numeric - mu)\n",
    "X_norm = (X_Numeric - mu) / sigma\n",
    "\n",
    "# Create scatter plots to visualize the standardization process\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "titles = [\"Unnormalized\", r\"X - $\\mu$\", \"Z-score Normalized\"]\n",
    "\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        data_x = X_Numeric.iloc[:, feature_indices[0]]\n",
    "        data_y = X_Numeric.iloc[:, feature_indices[1]]\n",
    "    elif i == 1:\n",
    "        data_x = X_mean.iloc[:, feature_indices[0]]\n",
    "        data_y = X_mean.iloc[:, feature_indices[1]]\n",
    "    else:\n",
    "        data_x = X_norm.iloc[:, feature_indices[0]]\n",
    "        data_y = X_norm.iloc[:, feature_indices[1]]\n",
    "    \n",
    "    ax[i].scatter(data_x, data_y)\n",
    "    ax[i].set_xlabel(X_Numeric.columns[feature_indices[0]])\n",
    "    ax[i].set_ylabel(X_Numeric.columns[feature_indices[1]])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].axis('equal')\n",
    "\n",
    "# Adjust layout and add a main title\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"Distribution of Features Before, During, After Normalization\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c595f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_Numeric)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_Numeric,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9b90f",
   "metadata": {},
   "source": [
    "<p> Values after normalization are closer in value than before. Next lets run a linear regression. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0d3a8",
   "metadata": {},
   "source": [
    "<h3> Linear Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining out training and test data. Setting test data = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2)\n",
    "\n",
    "# Set up model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Using statsmodel OLS to find our regression results \n",
    "X2 = sm.add_constant(X_norm)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47971c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    " \n",
    "# plotting residual errors in training data\n",
    "plt.scatter(linear_model.predict(X_train),\n",
    "            linear_model.predict(X_train) - y_train,\n",
    "            color=\"green\", s=10,\n",
    "            label='Train data')\n",
    " \n",
    "# plotting residual errors in test data\n",
    "plt.scatter(linear_model.predict(X_test),\n",
    "            linear_model.predict(X_test) - y_test,\n",
    "            color=\"blue\", s=10,\n",
    "            label='Test data')\n",
    " \n",
    "# plotting line for zero residual error\n",
    "plt.hlines(y=0, xmin=0, xmax=50, linewidth=2)\n",
    " \n",
    "# plotting legend\n",
    "plt.legend(loc='upper right')\n",
    " \n",
    "# plot title\n",
    "plt.title(\"Residual errors\")\n",
    " \n",
    "# method call for showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808c232",
   "metadata": {},
   "source": [
    "<p> Above you can see the results from the linear regression. Based on the R-Squared value, about 46.7% of the variance in our target variable is being explained by the independent variables. Looking at the F-statistic in combination with the probability, we can conclude that the model is very close to zero which suggests that the model overall is statistically significant.</p>\n",
    "<p> Overall this model can be improved - one way we can do that is to select more specific features </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95dd645",
   "metadata": {},
   "source": [
    "<h3> Feature Selection </h3>\n",
    "<p> Here we'll be running a Recursive Feature model to only keep a subset of the most relevant features from our dataset. This should improve the overall model performance, reduce overfitting, and help future algorithms </p>\n",
    "<p> RFE specifically uses machine learning models to remove the least important features until we are left with our desired number of features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining out training and test data. Setting test data = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2)\n",
    "\n",
    "# Set up model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Create an RFE (Recursive Feature Elimination) model and specify the number of features to select\n",
    "rfe = RFE(estimator=linear_model, n_features_to_select=10)\n",
    "\n",
    "# Fit the RFE model to your data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features =X_train.columns[rfe.support_]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57081c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only selected features\n",
    "columns_to_keep = selected_features\n",
    "\n",
    "# Keeping only RFE detected columns\n",
    "X_norm_rfe = X_norm[columns_to_keep]\n",
    "\n",
    "# Checking shape\n",
    "print(f\"X_norm Shape:{X_norm_rfe.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining out training and test data. Setting test data = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm_rfe, y, test_size=0.2)\n",
    "\n",
    "# Set up model\n",
    "linear_model_rfe = LinearRegression()\n",
    "linear_model_rfe.fit(X_train, y_train)\n",
    "\n",
    "# Using statsmodel OLS to find our regression results \n",
    "X2 = sm.add_constant(X_norm_rfe)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    " \n",
    "# plotting residual errors in training data\n",
    "plt.scatter(linear_model_rfe.predict(X_train),\n",
    "            linear_model_rfe.predict(X_train) - y_train,\n",
    "            color=\"green\", s=20,\n",
    "            label='Train data')\n",
    " \n",
    "# plotting residual errors in test data\n",
    "plt.scatter(linear_model_rfe.predict(X_test),\n",
    "            linear_model_rfe.predict(X_test) - y_test,\n",
    "            color=\"blue\", s=20,\n",
    "            label='Test data')\n",
    " \n",
    "# plotting line for zero residual error\n",
    "plt.hlines(y=0, xmin=0, xmax=50, linewidth=2)\n",
    " \n",
    "# plotting legend\n",
    "plt.legend(loc='upper right')\n",
    " \n",
    "# plot title\n",
    "plt.title(\"Residual errors\")\n",
    " \n",
    "# method call for showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19428175",
   "metadata": {},
   "source": [
    "<p> Investigating the removal of extra columns yielded minimal impact on the model's performance. To enhance the model further, consider generating new variables based on existing fields, such as 'days_from_release_date' derived from the 'release_day' variable. Additionally, you can explore the possibility of identifying top global artists and introducing indicator variables for each of these influential artists. For instance, does a song performed by Taylor Swift serve as a reliable predictor for higher or lower stream counts? While various options exist, we will continue with our current model for this specific analysis, given its significant performance. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9628fce",
   "metadata": {},
   "source": [
    "<h2> Gradient Descent </h2>\n",
    "<p> Next we can try a different kind of model which is called Gradient Descent. The overall purpose of this model is to optimize a function. It minimizes the cost function while at the same time adjusting the model's parameters bit by bit. For this example I will continue to use the normalized target and feature variables, but I will go back to using all the features that were included prior to utilizing RFE. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X : dropping variables that are not of interest at the moment\n",
    "X = df.drop(['track_name', 'artist(s)_name','streams','released_year','released_month','released_day'], axis = 1)\n",
    "X_Numeric = (X.loc[:, :'speechiness_%'])\n",
    "\n",
    "# Define y - using log transformation due to large values\n",
    "df['streams_log'] = np.log(df['streams'])\n",
    "y = df.streams_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_Numeric)\n",
    "\n",
    "# Defining out training and test data. Setting test data = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2)\n",
    "\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_Numeric,axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86819516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit regression model\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_train, y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093fec9",
   "metadata": {},
   "source": [
    "<p> The above results state that the model went through 17 iterations and updated the parameters 11085 times </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a12e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at parameters from model using normalized values\n",
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using sgdr.predict()\n",
    "y_pred_sgd = sgdr.predict(X_test)\n",
    "\n",
    "# make a prediction using w,b. \n",
    "y_pred = np.dot(X_test, w_norm) + b_norm  \n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40770667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig, ax = plt.subplots(1, 6, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:, i], y_train, label='target')\n",
    "    ax[i].set_xlabel(f'Feature {i}')\n",
    "    ax[i].scatter(X_test[:, i], y_pred, color=\"orange\", label='predict')\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"Target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a29b34",
   "metadata": {},
   "source": [
    "<p> So these are the results of our gradient descent model. Let's break them down! </p>\n",
    "<p> <b>MSE</b> : measures the average squared difference between the predicted values and the actual target values. A lower MSE = better predictions. My current model has quite a bit of error when compared to the actual targets </p>\n",
    "<p> <b>RMSE</b> : is simply the square root of the MSE  - very similar interpretation to the above example. </p>\n",
    "<p> <b>MAE</b> : This measures the absolute difference between the predicted values and the actual target values. We also want a lower value here. </p>\n",
    "<p><b>R2</b> : This measures how well the model explains the variance in the target variables. This model explains about 50% of the variability in the target variable. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2fb1f",
   "metadata": {},
   "source": [
    "<h2>Concluding Thoughts</h2>\n",
    "<p> In summary, the predictive capability of this model can be described as average at best. However, there is ample room for enhancement, which I intend to explore gradually. This endeavor will involve the thoughtful incorporation of additional high-quality variables into the dataset and addressing outliers, a few of which are present in the data, with the aim of enhancing the model's effectiveness. </p>\n",
    "<p> The primary aim of this analysis was twofold: to gain exposure to this dataset and to apply a diverse array of data analysis techniques. Throughout this exploration, I leveraged various functions to achieve comparable results. This approach reflects my curiosity and serves as a means to assess the variability of results across different tools. It also plays a pivotal role in gauging my progress and comprehension of the subject matter.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5aa10a",
   "metadata": {},
   "source": [
    "<h4>Feedback</h4>\n",
    "<p> I love learning, but with learning you have success and you most definetely have failures. Although I'm pretty well aquainted with the tools used in this analysis, if you find an error or have a suggestion, please do not hesistate to reach out! You can find my information on my webpage. Thank you! <p>\n",
    "\n",
    "<p>Data Sourced from Kaggle : <a>https://www.kaggle.com/datasets/nelgiriyewithana/top-spotify-songs-2023/data</a><p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
